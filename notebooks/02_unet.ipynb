{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fb8fd7",
   "metadata": {},
   "source": [
    "# U-Net for Cell Segmentation\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MateiGoidan/CellSeg/blob/main/notebooks/02_unet.ipynb)\n",
    "\n",
    "---\n",
    "## 1. Setup\n",
    "Import dependencies and configure environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_G3DDgKQI7-",
    "outputId": "87ba2103-7666-4ec4-83aa-90f34b70eace"
   },
   "outputs": [],
   "source": [
    "# @title Conectare la drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f390--3oP-_t"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "!pip install tensorflow-addons\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0siiGoIDQReA"
   },
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI8O-K2aQUak"
   },
   "outputs": [],
   "source": [
    "# @title Path-uri\n",
    "\n",
    "train_img_path = \"/content/drive/MyDrive/Facultate/Licenta/ds1/train/img/cls\"\n",
    "train_bin_mask_path = \"/content/drive/MyDrive/Facultate/Licenta/ds1/train/bin_mask/cls\"\n",
    "train_mult_mask_path = \"/content/drive/MyDrive/Facultate/Licenta/ds1/train/mult_mask/cls\"\n",
    "\n",
    "validation_img_path = \"/content/drive/MyDrive/Facultate/Licenta/ds1/validation/img/cls\"\n",
    "validation_bin_mask_path = \"/content/drive/MyDrive/Facultate/Licenta/ds1/validation/bin_mask/cls\"\n",
    "validation_mult_mask_path = \"/content/drive/MyDrive/Facultate/Licenta/ds1/validation/mult_mask/cls\"\n",
    "\n",
    "test_img_path = \"/content/drive/MyDrive/Facultate/Licenta/ds1/test/img/cls\"\n",
    "test_mask_path = \"/content/drive/MyDrive/Facultate/Licenta/ds1/test/mult_mask/cls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIPstvDzQeaK"
   },
   "outputs": [],
   "source": [
    "# @title Incarcam path-urile\n",
    "\n",
    "def load_paths(imd_dir, mask_dir):\n",
    "  image_paths = sorted([os.path.join(imd_dir, file_name) for file_name in os.listdir(imd_dir) if file_name.endswith(\".tif\")])\n",
    "  mask_paths = sorted([os.path.join(mask_dir, file_name) for file_name in os.listdir(mask_dir) if file_name.endswith(\".tif\")])\n",
    "  return image_paths, mask_paths\n",
    "\n",
    "train_img_paths, train_mask_paths = load_paths(train_img_path, train_mult_mask_path)\n",
    "\n",
    "validation_img_paths, validation_mask_paths = load_paths(validation_img_path, validation_mult_mask_path)\n",
    "\n",
    "test_imgs_paths, test_masks_paths = load_paths(test_img_path, test_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaL5psXHQzzA"
   },
   "outputs": [],
   "source": [
    "# @title Augumentare data\n",
    "\n",
    "def augment_data(image, mask):\n",
    "  # Random flip orizontal\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    mask = tf.image.flip_left_right(mask)\n",
    "\n",
    "  # Random flip vertical\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    image = tf.image.flip_up_down(image)\n",
    "    mask = tf.image.flip_up_down(mask)\n",
    "\n",
    "  # Random lumina\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "\n",
    "  # Random contrast\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "\n",
    "  # Random rotire\n",
    "  angle = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "  image = tf.image.rot90(image, angle)\n",
    "  mask = tf.image.rot90(mask, angle)\n",
    "\n",
    "  return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppmDBwdMQ08E"
   },
   "outputs": [],
   "source": [
    "# @title Preprocesare imagine + masca\n",
    "\n",
    "def load_tif_with_pil(path):\n",
    "  img = Image.open(path.numpy().decode('utf-8'))\n",
    "  img = img.convert('L') # Convertim la grayscale\n",
    "  img = np.array(img, dtype=np.float32)\n",
    "  return img\n",
    "\n",
    "def preprocess_mult(img_path, mask_path):\n",
    "  img = tf.py_function(load_tif_with_pil, [img_path], tf.float32)\n",
    "  img = tf.expand_dims(img, axis=-1)\n",
    "  img.set_shape([None, None, 1])\n",
    "  img = tf.image.resize(img, (256, 256))\n",
    "  img = (img / 255.0) * 2.0 - 1.01 # Normalizare in intervalul [-1, 1]\n",
    "\n",
    "  mask = tf.py_function(load_tif_with_pil, [mask_path], tf.float32)\n",
    "  mask = tf.expand_dims(mask, axis=-1)\n",
    "  mask.set_shape([None, None, 1])\n",
    "  mask = tf.image.resize(mask, (256, 256), method='nearest')\n",
    "  mask = tf.squeeze(mask, axis=-1)          # Scoatem dimensiunea suplimentară\n",
    "  mask = tf.cast(mask, tf.uint8)\n",
    "  mask = tf.one_hot(mask, depth=8)           # Convertim la one-hot (8 clase)\n",
    "  mask = tf.cast(mask, tf.float32)           # TensorFlow vrea float pentru loss\n",
    "\n",
    "  # Setam shape explicit ca in paper\n",
    "  img.set_shape([256, 256, 1])\n",
    "  mask.set_shape([256, 256, 8])\n",
    "\n",
    "  return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laZjda3DQ47c",
    "outputId": "7f31e6a4-7bb0-464b-9b4b-c4c4934c42b6"
   },
   "outputs": [],
   "source": [
    "# @title Creare Dataset\n",
    "\n",
    "def create_dataset(img_paths, mask_paths, batch_size=8, shuffle=True, augment=False):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((img_paths, mask_paths))\n",
    "  dataset = dataset.map(preprocess_mult, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "\n",
    "  if augment:\n",
    "    dataset = dataset.map(lambda img, mask: augment_data(img, mask), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "train_dataset_unet = create_dataset(train_img_paths, train_mask_paths, batch_size=8, augment=True)\n",
    "\n",
    "validation_dataset_unet = create_dataset(validation_img_paths, validation_mask_paths, batch_size=8, augment=False)\n",
    "\n",
    "test_dataset_unet = create_dataset(test_imgs_paths, test_masks_paths, batch_size=8, shuffle=False, augment=False)\n",
    "\n",
    "print(train_dataset_unet.cardinality().numpy())\n",
    "print(validation_dataset_unet.cardinality().numpy())\n",
    "print(test_dataset_unet.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "arGPABRoSr8a",
    "outputId": "0f585389-08c9-470b-b479-a35ef417d184"
   },
   "outputs": [],
   "source": [
    "# Definim culori pentru fiecare clasă (8 culori)\n",
    "clase_culori = np.array([\n",
    "    [0, 0, 0],       # Background - negru\n",
    "    [255, 0, 0],     # Rod - roșu\n",
    "    [0, 255, 0],     # RBC/WBC - verde\n",
    "    [0, 0, 255],     # Yeast - albastru\n",
    "    [255, 255, 0],   # Miscellaneous - galben\n",
    "    [255, 0, 255],   # Single EPC - magenta\n",
    "    [0, 255, 255],   # Small EPC sheet - cyan\n",
    "    [255, 165, 0]    # Large EPC sheet - portocaliu\n",
    "], dtype=np.uint8)\n",
    "\n",
    "# Funcție de conversie de la one-hot la imagine colorată\n",
    "def decode_mask_one_hot(mask_one_hot):\n",
    "  mask_labels = np.argmax(mask_one_hot, axis=-1)  # (H, W)\n",
    "  mask_rgb = clase_culori[mask_labels]            # (H, W, 3)\n",
    "  return mask_rgb\n",
    "\n",
    "# Selectăm un batch din dataset\n",
    "for img_batch, mask_batch in train_dataset_unet.take(1):\n",
    "  img = img_batch[0].numpy()   # Imagine\n",
    "  mask = mask_batch[0].numpy() # Mască one-hot\n",
    "  break\n",
    "\n",
    "# Decodăm masca\n",
    "decoded_mask = decode_mask_one_hot(mask)\n",
    "\n",
    "# Plotăm imaginea și masca\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Imaginea originală\n",
    "axes[0].imshow((img + 1) / 2, cmap='gray')  # Reconvertim din [-1,1] în [0,1]\n",
    "axes[0].set_title('Imagine originală')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Masca colorată\n",
    "axes[1].imshow(decoded_mask)\n",
    "axes[1].set_title('Mască one-hot colorată')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkkzoYd9U_UP"
   },
   "source": [
    "# Loss + Metrici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJKqR5tOTrWG"
   },
   "outputs": [],
   "source": [
    "# @title Loss: Focal Loss + Multiclass Dice Loss\n",
    "\n",
    "def focal_loss(class_weights, gamma=2.0):\n",
    "  class_weights = tf.constant(class_weights, dtype=tf.float32)\n",
    "\n",
    "  def loss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1. - K.epsilon())\n",
    "    cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "    weight = class_weights * y_true\n",
    "    focal = tf.math.pow(1 - y_pred, gamma) * cross_entropy * weight\n",
    "    return tf.reduce_mean(tf.reduce_sum(focal, axis=-1))  # medie pe batch.\n",
    "  return loss\n",
    "\n",
    "def multiclass_dice_loss():\n",
    "  def loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1. - K.epsilon())\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[1,2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "  return loss\n",
    "\n",
    "def combined_loss(class_weights, gamma=2.0, weight_dice=1.0):\n",
    "  focal_fn = focal_loss(class_weights, gamma)\n",
    "  dice_fn = multiclass_dice_loss()\n",
    "  def loss(y_true, y_pred):\n",
    "    return focal_fn(y_true, y_pred) + weight_dice * dice_fn(y_true, y_pred)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwjqlOAbVCls"
   },
   "outputs": [],
   "source": [
    "# @title Metrici personalizati\n",
    "\n",
    "def iou_multiclass(y_true, y_pred):\n",
    "  # Convertim predictia softmax in argmax\n",
    "  y_pred = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "\n",
    "  # y_true este deja one-hot din dataset. Convertim inapoi la indici pentru comparatie.\n",
    "  y_true = tf.argmax(y_true, axis=-1, output_type=tf.int32)\n",
    "\n",
    "  intersection = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "  union = tf.reduce_sum(tf.ones_like(y_true, dtype=tf.float32))\n",
    "\n",
    "  return intersection / (union + K.epsilon())\n",
    "\n",
    "def dice_multiclass(y_true, y_pred):\n",
    "  # y_true este deja one-hot din dataset, shape (batch_size, H, W, num_classes)\n",
    "  # y_pred are shape (batch_size, H, W, num_classes) dupa softmax\n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "  # Calculeaza intersectia si suma de pixeli pentru fiecare clasa\n",
    "  # Reduce sum pe dimensiunile spatiale (H, W)\n",
    "  intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2]) # shape (batch_size, num_classes)\n",
    "  denominator = tf.reduce_sum(y_true + y_pred, axis=[1, 2]) # shape (batch_size, num_classes)\n",
    "\n",
    "  # Calculeaza dice score per clasa, per imagine in batch\n",
    "  dice_per_class_per_image = (2.0 * intersection + K.epsilon()) / (denominator + K.epsilon()) # shape (batch_size, num_classes)\n",
    "\n",
    "  # Returneaza media dice score pe tot batch-ul (media pe imagini si pe clase)\n",
    "  return tf.reduce_mean(dice_per_class_per_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_od_5TmnUZEb"
   },
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbePFj1yHNdA"
   },
   "outputs": [],
   "source": [
    "# @title Instance Normalization manuala\n",
    "# InstanceNormalization din TensorFlow Addons din moment ce nu mai e compatibil\n",
    "# Respecta modelul din paper\n",
    "\n",
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "  def __init__(self, epsilon=1e-5, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.gamma = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=\"ones\",\n",
    "                                 trainable=True)\n",
    "    self.beta = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=\"zeros\",\n",
    "                                trainable=True)\n",
    "    super(InstanceNormalization, self).build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n",
    "    inv = tf.sqrt(variance + self.epsilon)\n",
    "    normalized = (inputs - mean) / inv\n",
    "    return self.gamma * normalized + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wn8_AW8RUdJd"
   },
   "outputs": [],
   "source": [
    "# @title U-Net Multi-Class\n",
    "\n",
    "def conv_block(inputs, filters, dropout_rate=0.2):\n",
    "  x = layers.Conv2D(filters, 3, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "  x = InstanceNormalization()(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "\n",
    "  x = layers.Conv2D(filters, 3, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                    kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "  x = InstanceNormalization()(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "\n",
    "  x = layers.SpatialDropout2D(dropout_rate)(x)\n",
    "  return x\n",
    "\n",
    "def build_unet_multiclass(input_shape=(256,256,1), num_classes=8, dropout_rate=0.2):\n",
    "  inputs_tensor = layers.Input(shape=input_shape)\n",
    "\n",
    "  # Encoder\n",
    "  c1 = conv_block(inputs_tensor, 32, dropout_rate)\n",
    "  p1 = layers.MaxPooling2D()(c1)\n",
    "\n",
    "  c2 = conv_block(p1, 64, dropout_rate)\n",
    "  p2 = layers.MaxPooling2D()(c2)\n",
    "\n",
    "  c3 = conv_block(p2, 128, dropout_rate)\n",
    "  p3 = layers.MaxPooling2D()(c3)\n",
    "\n",
    "  c4 = conv_block(p3, 256, dropout_rate)\n",
    "  p4 = layers.MaxPooling2D()(c4)\n",
    "\n",
    "  # Bottleneck\n",
    "  c5 = conv_block(p4, 512, dropout_rate)\n",
    "\n",
    "  # Decoder\n",
    "  u6 = layers.Conv2DTranspose(256, 2, strides=2, padding=\"same\")(c5)\n",
    "  u6 = layers.concatenate([u6, c4])\n",
    "  c6 = conv_block(u6, 256, dropout_rate)\n",
    "\n",
    "  u7 = layers.Conv2DTranspose(128, 2, strides=2, padding=\"same\")(c6)\n",
    "  u7 = layers.concatenate([u7, c3])\n",
    "  c7 = conv_block(u7, 128, dropout_rate)\n",
    "\n",
    "  u8 = layers.Conv2DTranspose(64, 2, strides=2, padding=\"same\")(c7)\n",
    "  u8 = layers.concatenate([u8, c2])\n",
    "  c8 = conv_block(u8, 64, dropout_rate)\n",
    "\n",
    "  u9 = layers.Conv2DTranspose(32, 2, strides=2, padding=\"same\")(c8)\n",
    "  u9 = layers.concatenate([u9, c1])\n",
    "  c9 = conv_block(u9, 32, dropout_rate)\n",
    "\n",
    "  # Output\n",
    "  outputs = layers.Conv2D(num_classes, 1, activation=\"softmax\")(c9)\n",
    "\n",
    "  model = tf.keras.Model(inputs=inputs_tensor, outputs=outputs, name=\"U-Net_Multiclass\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "iEnBYC4jUnQo",
    "outputId": "50d77ed8-d4c6-4080-a558-71104a12557c"
   },
   "outputs": [],
   "source": [
    "# @title Compile\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "class_weights = [1, 20, 5, 15, 10, 1, 5, 15]\n",
    "\n",
    "unet_model = build_unet_multiclass()\n",
    "unet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=combined_loss(class_weights, gamma=2.0, weight_dice=1.0),\n",
    "    # loss='categorical_crossentropy',\n",
    "    metrics=[MeanIoU(num_classes=8), dice_multiclass]\n",
    ")\n",
    "\n",
    "\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "0Y1BbZP7NIYi",
    "outputId": "0f25c6fc-95cb-4928-de4d-478e32d154a2"
   },
   "outputs": [],
   "source": [
    "# @title Verificare distributiei claselor in predictie\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def clase_detectate(pred, denumiri_clase=None):\n",
    "  \"\"\"\n",
    "  pred: tensorul returnat de model, dimensiune (1, H, W, C)\n",
    "  denumiri_clase: listă opțională cu numele claselor\n",
    "  \"\"\"\n",
    "  pred_argmax = np.argmax(pred[0], axis=-1)  # extragem clasa cu probabilitatea cea mai mare per pixel\n",
    "  clase_unice, pixeli = np.unique(pred_argmax, return_counts=True)\n",
    "\n",
    "  print(\"Distribuție pixeli în predicție:\")\n",
    "  for i, count in zip(clase_unice, pixeli):\n",
    "    nume = denumiri_clase[i] if denumiri_clase else f\"Clasa {i}\"\n",
    "    print(f\"{nume}: {count} pixeli\")\n",
    "\n",
    "pred = unet_model.predict(image_batch)  # sau o singură imagine reshape-uită (1, 256, 256, 1)\n",
    "clase_detectate(pred, denumiri_clase=[\n",
    "  \"Background\", \"Rod\", \"RBC/WBC\", \"Yeast\", \"Misc\", \"Single EPC\", \"Small EPC sheet\", \"Large EPC sheet\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZOvcOA3UrXs",
    "outputId": "159366e7-bc19-4951-bab4-e8dbf25596c2"
   },
   "outputs": [],
   "source": [
    "# @title Antrenare\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "training = unet_model.fit(\n",
    "    train_dataset_unet,\n",
    "    validation_data=validation_dataset_unet,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "NjLJc4RylnPu",
    "outputId": "fdbdd310-aeb1-425f-c67f-2f3b94bb2d24"
   },
   "outputs": [],
   "source": [
    "# Salvare model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18,5)) # Increased figure size to accommodate more plots\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(training.history['loss'], label='Train Loss')\n",
    "plt.plot(training.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(training.history['mean_io_u_3'], label='Train Mean IoU') # Use the metric name added in compile\n",
    "plt.plot(training.history['val_mean_io_u_3'], label='Validation Mean IoU') # Use the metric name added in compile\n",
    "plt.title('Mean IoU')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(training.history['dice_multiclass'], label='Train Dice') # Use the metric name added in compile\n",
    "plt.plot(training.history['val_dice_multiclass'], label='Validation Dice') # Use the metric name added in compile\n",
    "plt.title('Dice Coefficient')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKYconbOJzQc"
   },
   "source": [
    "# Evaluare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MF_YeXugJ30b",
    "outputId": "9c4c0d79-8ed7-494f-ef23-1a3c3859de1b"
   },
   "outputs": [],
   "source": [
    "test_results = unet_model.evaluate(test_dataset_unet, verbose=1)\n",
    "\n",
    "print(\"Test Loss:\", test_results[0])\n",
    "print(\"IoU:\", test_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tahebQgPTDAM",
    "outputId": "0d18b1b5-aaef-4a49-fbba-4157485cbd0e"
   },
   "outputs": [],
   "source": [
    "def calculate_dice_per_class(model, dataset, num_classes=8, smooth=1e-6):\n",
    "  intersection = np.zeros(num_classes)\n",
    "  union = np.zeros(num_classes)\n",
    "\n",
    "  for img_batch, mask_batch in dataset:\n",
    "    preds = model.predict(img_batch)\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    masks = np.argmax(mask_batch.numpy(), axis=-1)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "      pred_cls = (preds == cls).astype(np.float32)\n",
    "      mask_cls = (masks == cls).astype(np.float32)\n",
    "\n",
    "      intersection[cls] += np.sum(pred_cls * mask_cls)\n",
    "      union[cls] += np.sum(pred_cls) + np.sum(mask_cls)\n",
    "\n",
    "  dice_per_class = (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "  print(\"=== Dice Score per clasă ===\")\n",
    "  for i, score in enumerate(dice_per_class):\n",
    "    print(f\"Clasă {i}: {score:.4f}\")\n",
    "\n",
    "  return dice_per_class\n",
    "\n",
    "# Apelează funcția\n",
    "dice_scores = calculate_dice_per_class(unet_model, test_dataset_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w83I74NGK82a",
    "outputId": "a49d8d1e-9034-4e32-f0b9-3de4d1529bc7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Culori RGB pentru cele 8 clase\n",
    "clase_culori = np.array([\n",
    "    [0, 0, 0],       # 0 - Background\n",
    "    [255, 0, 0],     # 1 - Rod\n",
    "    [0, 255, 0],     # 2 - RBC/WBC\n",
    "    [0, 0, 255],     # 3 - Yeast\n",
    "    [255, 255, 0],   # 4 - Misc\n",
    "    [255, 0, 255],   # 5 - Single EPC\n",
    "    [0, 255, 255],   # 6 - Small EPC sheet\n",
    "    [255, 165, 0]    # 7 - Large EPC sheet\n",
    "], dtype=np.uint8)\n",
    "\n",
    "def decode_mask(mask_one_hot):\n",
    "  mask_labels = np.argmax(mask_one_hot, axis=-1)\n",
    "  return clase_culori[mask_labels]\n",
    "\n",
    "def vizualizeaza_predictii(model, dataset, nume_dataset=\"Test Set\", nume_model=\"Model\", nume_pred=\"Predicție\", n=5):\n",
    "  fig, axes = plt.subplots(n, 3, figsize=(15, 5 * n))\n",
    "  for i, (img_batch, mask_batch) in enumerate(dataset.take(n)):\n",
    "    img = img_batch[0].numpy()\n",
    "    mask_true = mask_batch[0].numpy()\n",
    "\n",
    "    pred = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "    img_display = ((img + 1) / 2).squeeze()\n",
    "\n",
    "    mask_true_rgb = decode_mask(mask_true)\n",
    "    mask_pred_rgb = decode_mask(pred)\n",
    "\n",
    "    axes[i, 0].imshow(img_display, cmap='gray')\n",
    "    axes[i, 0].set_title(f\"{nume_dataset} - Imagine {i+1}\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(mask_true_rgb)\n",
    "    axes[i, 1].set_title(\"Ground Truth\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    axes[i, 2].imshow(mask_pred_rgb)\n",
    "    axes[i, 2].set_title(nume_pred)\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# Apelează funcția\n",
    "vizualizeaza_predictii(unet_model, test_dataset_unet, n=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
